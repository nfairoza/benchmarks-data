https://www.intel.com/content/dam/develop/external/us/en/documents/java-tuning-guide-on-xeon.pdf
https://community.arm.com/arm-community-blogs/b/infrastructure-solutions-blog/posts/performance-of-specjbb2015-on-oci-ampere-a1-compute-instances

ARM: 80 cores 
-server 
-Xms124g 
-Xmx124g 
-Xmn114g 
-XX:SurvivorRatio=20 
-XX:MaxTenuringThreshold=15 
-XX:+UseLargePages 
-XX:LargePageSizeInBytes=2m 
-XX:+UseParallelGC 
-XX:+AlwaysPreTouch 
-XX:-UseAdaptiveSizePolicy 
-XX:-UsePerfData 
-XX:ParallelGCThreads=40 
-XX:+UseTransparentHugePages 
-XX:+UseCompressedOops 
-XX:ObjectAlignmentInBytes=32

ARM specjbb2015 options:
Dspecjbb.comm.connect.timeouts.connect=700000 
-Dspecjbb.comm.connect.timeouts.read=700000 
-Dspecjbb.comm.connect.timeouts.write=700000 
-Dspecjbb.customerDriver.threads.probe=80 
-Dspecjbb.forkjoin.workers.Tier1=80 
-Dspecjbb.forkjoin.workers.Tier2=1 
-Dspecjbb.forkjoin.workers.Tier3=16
-Dspecjbb.heartbeat.period=100000
-Dspecjbb.heartbeat.threshold=1000000
---
AMD:
AMD specjbb2015 tunables:
SPEC_OPTS_C="
-Dspecjbb.group.count=$GROUP_COUNT 
-Dspecjbb.txi.pergroup.count=$TI_JVM_COUNT 
-Dspecjbb.heartbeat.period=2000 
-Dspecjbb.heartbeat.threshold=9000000 
-Dspecjbb.controller.handshake.timeout=900000 
-Dspecjbb.controller.handshake.period=20000 
-Dspecjbb.forkjoin.workers.Tier1=140 
-Dspecjbb.forkjoin.workers.Tier2=1 
-Dspecjbb.forkjoin.workers.Tier3=20 
-Dspecjbb.comm.connect.selector.runner.count=16 
-Dspecjbb.mapreducer.pool.size=64"

SPEC_OPTS_TI="
-Dspecjbb.comm.connect.selector.runner.count=8 
-Dspecjbb.comm.connect.worker.pool.min=4 
-Dspecjbb.comm.connect.worker.pool.max=150 
-Dspecjbb.comm.connect.client.pool.size=150"

SPEC_OPTS_BE="
-Dspecjbb.comm.connect.selector.runner.count=8 
-Dspecjbb.comm.connect.worker.pool.min=4 
-Dspecjbb.comm.connect.worker.pool.max=150 
-Dspecjbb.comm.connect.client.pool.size=150"
---
tuned-adm profile throughput-performance

---Store default setting---
# Save current kernel values
def_sched_rt_runtime_us=`cat /proc/sys/kernel/sched_rt_runtime_us`
def_sched_latency_ns=`cat /proc/sys/kernel/sched_latency_ns`
def_sched_migration_cost_ns=`cat /proc/sys/kernel/sched_migration_cost_ns`
def_sched_min_granularity_ns=`cat /proc/sys/kernel/sched_min_granularity_ns`
def_sched_wakeup_granularity_ns=`cat /proc/sys/kernel/sched_wakeup_granularity_ns`
def_dirty_expire_centisecs=`cat /proc/sys/vm/dirty_expire_centisecs`
def_dirty_writeback_centisecs=`cat /proc/sys/vm/dirty_writeback_centisecs`
def_dirty_ratio=`cat /proc/sys/vm/dirty_ratio`
def_dirty_background_ratio=`cat /proc/sys/vm/dirty_background_ratio`
def_swappiness=`cat /proc/sys/vm/swappiness`
def_numa_stat=`cat  /proc/sys/vm/numa_stat`
def_numa_balancing=`cat /proc/sys/kernel/numa_balancing`
def_thp_enabled=`cat /sys/kernel/mm/transparent_hugepage/enabled | sed 's/.*\[\([^]]*\)].*/\1/'`
def_thp_defrag=`cat /sys/kernel/mm/transparent_hugepage/defrag | sed 's/.*\[\([^]]*\)].*/\1/'`

---Set tunables for duration of benchmark----
echo 950000 > /proc/sys/kernel/sched_rt_runtime_us
echo 100000000 > /proc/sys/kernel/sched_latency_ns
echo 80000 > /proc/sys/kernel/sched_migration_cost_ns
echo 600000 > /proc/sys/kernel/sched_min_granularity_ns
echo 100000 > /proc/sys/kernel/sched_wakeup_granularity_ns

echo 10000 > /proc/sys/vm/dirty_expire_centisecs
echo 1500 > /proc/sys/vm/dirty_writeback_centisecs
echo 40 > /proc/sys/vm/dirty_ratio
echo 10 > /proc/sys/vm/dirty_background_ratio
echo 10 > /proc/sys/vm/swappiness
echo 0 > /proc/sys/vm/numa_stat

echo 0 > /proc/sys/kernel/numa_balancing
echo always > /sys/kernel/mm/transparent_hugepage/enabled
echo always > /sys/kernel/mm/transparent_hugepage/defrag

----Restore the defaults back after benchmark-----
echo $def_sched_rt_runtime_us > /proc/sys/kernel/sched_rt_runtime_us
echo $def_sched_latency_ns > /proc/sys/kernel/sched_latency_ns
echo $def_sched_migration_cost_ns > /proc/sys/kernel/sched_migration_cost_ns
echo $def_sched_min_granularity_ns > /proc/sys/kernel/sched_min_granularity_ns
echo $def_sched_wakeup_granularity_ns > /proc/sys/kernel/sched_wakeup_granularity_ns
echo $def_dirty_expire_centisecs > /proc/sys/vm/dirty_expire_centisecs
echo $def_dirty_writeback_centisecs > /proc/sys/vm/dirty_writeback_centisecs
echo $def_dirty_ratio > /proc/sys/vm/dirty_ratio
echo $def_dirty_background_ratio > /proc/sys/vm/dirty_background_ratio
echo $def_swappiness > /proc/sys/vm/swappiness
echo $def_numa_stat >  /proc/sys/vm/numa_stat
echo $def_numa_balancing > /proc/sys/kernel/numa_balancing
echo $def_thp_enabled > /sys/kernel/mm/transparent_hugepage/enabled
echo $def_thp_defrag > /sys/kernel/mm/transparent_hugepage/defrag
------
-Xms27g
-Xmx27g
-Xmn25g
-XX:AllocatePrefetchInstr=2 
-XX:+UseParallelGC 
-XX:ParallelGCThreads=16 
-XX:LargePageSizeInBytes=2m 
-XX:-UseAdaptiveSizePolicy 
-XX:+AlwaysPreTouch 
-XX:+UseLargePages 
-XX:SurvivorRatio=28 
-XX:TargetSurvivorRatio=95 
-XX:MaxTenuringThreshold=15 
-XX:InlineSmallCode=11k 
-XX:MaxGCPauseMillis=300 
-XX:LoopUnrollLimit=200 
-XX:AdaptiveSizeMajorGCDecayTimeScale=12 
-XX:AdaptiveSizeDecrementScaleFactor=2 
-XX:+UseTransparentHugePages 
-XX:TLABAllocationWeight=55 
-XX:ThreadStackSize=512

Intel:
The Linux OS by may have many services running that may not be required for all scenarios. These services can impact
throughput and memory usage and can also be a hidden source of unpredictable latency. For SPECjbb 2015 we
recommend disabling the following.
systemctl stop systemd-update-utmp-runlevel.service

Intel recommended OS. Tunables:
sysctl net.core.wmem_max=12582912
sysctl net.core.rmem_max=12582912
sysctl net.ipv4.tcp_rmem='10240 87380 12582912'
sysctl net.ipv4.tcp_wmem='10240 87380 12582912'
sysctl net.core.netdev_max_backlog=655560
sysctl net.core.somaxconn=32768
sysctl net.ipv4.tcp_no_metrics_save=1
----
echo 10000 > /proc/sys/kernel/sched_cfs_bandwidth_slice_us
echo 0 > /proc/sys/kernel/sched_child_runs_first
echo 16000000 > /proc/sys/kernel/sched_latency_ns
echo 1000 > /proc/sys/kernel/sched_migration_cost_ns
echo 28000000 > /proc/sys/kernel/sched_min_granularity_ns
echo 9 > /proc/sys/kernel/sched_nr_migrate
echo 100 > /proc/sys/kernel/sched_rr_timeslice_ms
echo 1000000 > /proc/sys/kernel/sched_rt_period_us
echo 990000 > /proc/sys/kernel/sched_rt_runtime_us
echo 0 > /proc/sys/kernel/sched_schedstats
echo 1 > /proc/sys/kernel/sched_tunable_scaling
echo 50000000 > /proc/sys/kernel/sched_wakeup_granularity_ns
echo 3000 > /proc/sys/vm/dirty_expire_centisecs
echo 500 > /proc/sys/vm/dirty_writeback_centisecs
echo 40 > /proc/sys/vm/dirty_ratio
echo 10 > /proc/sys/vm/dirty_background_ratio
echo 10 > /proc/sys/vm/swappiness
---
 -XX:+UseLargePages
echo ** > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
---
-XX:+UseTransparentHugePages
echo always > /sys/kernel/mm/transparent_hugepage/defrag
echo always > /sys/kernel/mm/transparent_hugepage/enabled
----
echo 0 > /proc/sys/kernel/numa_balancing
---
SPECjbb 2015 uses a lot of memory, file descriptors, and sockets
ulimit -n 131072
ulimit -v 800000000
ulimit -m 800000000
----
> numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78
79
node 0 size: 128383 MB
node 0 free: 57497 MB
node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 80 81 82 83 84 85 86 87 88 89 90 91
92 93 94 95
node 1 size: 129017 MB
node 1 free: 59974 MB
node 2 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 96 97 98 99 100 101 102 103 104 105
106 107 108 109 110 111
node 2 size: 128979 MB
node 2 free: 58776 MB
node 3 cpus: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 112 113 114 115 116 117 118 119 120
121 122 123 124 125 126 127
node 3 size: 129014 MB
node 3 free: 59519 MB
ulimit -l 800000000
----
Note that each node’s “cpus” list contains two ranges, such as 0-15 and 64-79. The first range corresponds to the first
set of hyperthreads on the cores in the node, and the second range the remaining hyperthreads. So, on this hardware
“cpus” 0 and 64 are hyperthreads that share the same core. If it is desirable to further partition a node between
processes, it’s recommended that you avoid having processes share a core. For example, on this machine numactl with
the “--physcpubind” option should specify cpus 0-7,64-71 for one process and 8-15,72-79 for the other process.

The JVM has a UseNUMA option that optimizes memory locality when threads allocate Java objects. It works well on
NUMA systems when the JVM process has to span multiple NUMA nodes. It is recommended to not specify the
UseNUMA option on a system with only one node or with a Java process that is already allocated to a particular NUMA
node.
The recommendation for SPECjbb 2015 is to match the number of SPECjbb “groups” to the number of NUMA nodes in
the system, and use numactl to bind a BackEnd and Transaction Injector to each node:

# numactl --cpunodebind=$NewNode –localalloc <command to launch BE or TxI>
----
-XX:+UseLargePages option when using static huge pages, or alternatively you may want to try the -XX:+UseTransparentHugePages option. 
The -XX:LargePageSizeInBytes=1G option can be used to specify a requested huge page size.
For long running applications, we recommend using the -XX:+AlwaysPreTouch option to reduce latency at the expense
of startup time.-XX:+UseLargePages -XX:LargePageSizeInBytes=1G -XX:+AlwaysPreTouch

-XX:+UseLargePages
-XX:+UseTransparentHugePages
-XX:LargePageSizeInBytes=1G
-XX:+AlwaysPreTouch

----
The JVM option ParallelGCThreads specifies the maximum number of parallel GC threads used by ParallelGC and
G1GC. The JVM option ConcGCThreads specifies the maximum number of concurrent GC threads used by G1GC. The
default value for ConcGCThreads is ¼ ParallelGCThreads. We’ll focus on ParallelGCThreads for the rest of this section.

The default value of ParallelGCThreads for a Java process is approximately 63% the number of hyperthreads available
to the process. If the Java process is sharing a large machine (without being constrained to a subset of the
hyperthreads), this could result in way too many GC threads. This can compound if other processes on the machine are
also Java processes with too many GC threads. The effect of too many GC threads is the memory overhead of the
threads and the contention resulting from threads fighting over too little work to do (which can increase GC pauses).
By default, the JVM dynamically limits the actual number of parallel GC threads to account for the number of Java
application threads and current Java heap size:
= MIN( MAX((current number of Java threads * 2), (current Java heap size / HeapSizePerGCThread (32MB))), ParallelGCThreads)

Depending on the performance and number of vCPUs this could also result in too many GC threads.
If you specify ParallelGCThreads on the command line, then the actual number of parallel GC threads used is simply the
number specified (it’s not dynamically adjusted). The simplest work-around is, as part of allocating cpus to your Java processes using numactl (or similar), to also specify
the ParallelGCThreads option. If your application has a relatively large Java heap it is recommended that you set
ParallelGCThreads to the same value as the number of vCPUs that are dedicated to the Java process.

SPECjbb 2015 has a relatively small footprint for long-lived objects, and many short-lived objects. When focused on
throughput performance, using Parallel GC, a Java heap < 32GB (which enables the “CompressedOops” optimization), a
large young generation (-Xmn), and some manual size tuning results in better performance.

The number of ParallelGCThreads for each Backend should be set to the number of HW threads in the system (vCPUs)
divided by the number of groups. With an Intel® Xeon® Platinum 8380 the number of threads per NUMA node is 40
(with SNC enabled).

-XX:+UseParallelGC 
-XX:-UseAdaptiveSizePolicy 
-XX:SurvivorRatio=28 
-XX:TargetSurvivorRatio=95
-XX:MaxTenuringThreshold=15 
-verbose:gc 
----
-Xms31744m 
-Xmx31744m 
-Xmn29696m
---
-XX:ParallelGCThreads=XX (not set)
----
For large, long running Java applications like SPECjbb 2015 we recommend adjusting some of the JIT compiler and
software prefetching options for better performance.
-XX:InlineSmallCode=10k 
-XX:-UseCountedLoopSafepoints 
-XX:LoopUnrollLimit=20
-XX:AllocatePrefetchLines=3 
-XX:AllocateInstancePrefetchLines=2 
-XX:AllocatePrefetchStepSize=128
-XX:AllocatePrefetchDistance=384

---
Intel Specjbb options:
specjbb.controller.type = HBIR_RT
specjbb.controller.rtcurve.start = .0
specjbb.forkjoin.workers.Tier1=248
specjbb.forkjoin.workers.Tier2=8
specjbb.forkjoin.workers.Tier3=50
specjbb.group.count=4
specjbb.comm.connect.client.pool.size=232
specjbb.customerDriver.threads=75
specjbb.customerDriver.threads.probe=69
specjbb.customerDriver.threads.saturate=85
specjbb.mapreducer.pool.size=223
specjbb.comm.connect.selector.runner.count=1
specjbb.comm.connect.worker.pool.max=81
specjbb.comm.connect.worker.pool.min=24
specjbb.comm.connect.timeouts.connect=600000
specjbb.comm.connect.timeouts.read=600000
specjbb.comm.connect.timeouts.write=600000

-----
Intel Tool for measureing tput and latencies on Numa and Local memory nodes: 
https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html


